{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsfPgifi_CgV"
      },
      "source": [
        "Setting up basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAQg9Gx8ndXS",
        "outputId": "dfbdc5a1-7987-4744-9371-f4678312bd7b"
      },
      "outputs": [],
      "source": [
        "!pip install nes-py\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2YhdfCvinfu4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import numpy as np\n",
        "import collections \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj8fCcLr--oH"
      },
      "source": [
        "Gym and wrappers import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KGzhKrtq-X-x"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewO8PGGO-4Sl"
      },
      "source": [
        "Game Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mxpooX7l2t6m"
      },
      "outputs": [],
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ekQyXdpgBkAV"
      },
      "outputs": [],
      "source": [
        "state=env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERvtLqns2t6m"
      },
      "source": [
        "Render Game (works in VS code but not in Colab)\n",
        "(stops after 100 steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdEqBqyG2t6m"
      },
      "outputs": [],
      "source": [
        "# Create a flag - restart or not\n",
        "done = True\n",
        "# Loop through each frame in the game\n",
        "for step in range(100): \n",
        "    # Start the game to begin with \n",
        "    if done: \n",
        "        # Start the gamee\n",
        "        env.reset()\n",
        "    # Do random actions\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    # Show the game on the screen\n",
        "    env.render()\n",
        "# Close the game\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAq3kuIt_NmG"
      },
      "source": [
        "Preprocessing\n",
        "\n",
        "*   Skipping frames for our AI to learn from as consecutive frames may not offer too much relevant information\n",
        "*   Converting RGB frames to greyscale so that the AI has much less data to deal with\n",
        "*   Restricting the action space to basic 7 controls\n",
        "*   Normalising the pixel value in greyscale\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ch39ujr4nidl"
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(SkipFrame, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "class ProcessFrame(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Downsamples image to 84x84\n",
        "    Greyscales image\n",
        "\n",
        "    Returns numpy array\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 240 * 256 * 3:\n",
        "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "def make_env(env):\n",
        "    env = SkipFrame(env)\n",
        "    env = ProcessFrame(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    return JoypadSpace(env, SIMPLE_MOVEMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aAmWw1Uk2t6q"
      },
      "outputs": [],
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
        "env = make_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reduced Shape of The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzvsYpAz2t6q",
        "outputId": "56ca046e-1ceb-4f48-a980-2f3e436262e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 84, 84)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state=env.reset()\n",
        "state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn6n3VsCFcXU"
      },
      "source": [
        "DDQN algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3gka_3zDnnYQ"
      },
      "outputs": [],
      "source": [
        "class DDQNSolver(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DDQNSolver, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    \n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "    \n",
        "\n",
        "class DDQNAgent:\n",
        "\n",
        "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
        "                 dropout, exploration_max, exploration_min, exploration_decay, pretrained):\n",
        "\n",
        "        self.state_space = state_space\n",
        "        self.action_space = action_space\n",
        "        self.pretrained = pretrained\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.local_net = DDQNSolver(state_space, action_space).to(self.device)\n",
        "        self.target_net = DDQNSolver(state_space, action_space).to(self.device)\n",
        "            \n",
        "        if self.pretrained:\n",
        "            self.local_net.load_state_dict(torch.load(\"dq1.pt\", map_location=torch.device(self.device)))\n",
        "            self.target_net.load_state_dict(torch.load(\"dq2.pt\", map_location=torch.device(self.device)))\n",
        "                    \n",
        "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
        "        self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
        "        self.step = 0\n",
        "\n",
        "        # Create memory\n",
        "        self.max_memory_size = max_memory_size\n",
        "        if self.pretrained:\n",
        "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
        "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
        "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
        "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
        "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
        "            with open(\"ending_position.pkl\", 'rb') as f:\n",
        "                self.ending_position = pickle.load(f)\n",
        "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
        "                self.num_in_queue = pickle.load(f)\n",
        "        else:\n",
        "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
        "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
        "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
        "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
        "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
        "            self.ending_position = 0\n",
        "            self.num_in_queue = 0\n",
        "        \n",
        "        self.memory_sample_size = batch_size\n",
        "        \n",
        "        # Learning parameters\n",
        "        self.gamma = gamma\n",
        "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
        "        self.exploration_max = exploration_max\n",
        "        self.exploration_rate = exploration_max\n",
        "        self.exploration_min = exploration_min\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def remember(self, state, action, reward, state2, done):\n",
        "        self.STATE_MEM[self.ending_position] = state.float()\n",
        "        self.ACTION_MEM[self.ending_position] = action.float()\n",
        "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
        "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
        "        self.DONE_MEM[self.ending_position] = done.float()\n",
        "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
        "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
        "        \n",
        "    def recall(self):\n",
        "        # Randomly sample 'batch size' experiences\n",
        "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
        "        \n",
        "        STATE = self.STATE_MEM[idx]\n",
        "        ACTION = self.ACTION_MEM[idx]\n",
        "        REWARD = self.REWARD_MEM[idx]\n",
        "        STATE2 = self.STATE2_MEM[idx]\n",
        "        DONE = self.DONE_MEM[idx]\n",
        "        \n",
        "        return STATE, ACTION, REWARD, STATE2, DONE\n",
        "\n",
        "    def act(self, state):\n",
        "        # Epsilon-greedy action\n",
        "        \n",
        "        self.step += 1\n",
        "        if random.random() < self.exploration_rate:  \n",
        "            return torch.tensor([[random.randrange(self.action_space)]])\n",
        "        # Local net is used for the policy\n",
        "        return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
        "\n",
        "    def copy_model(self):\n",
        "        # Copy local net weights into target net\n",
        "        \n",
        "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
        "    \n",
        "    def experience_replay(self):\n",
        "        \n",
        "        if self.step % self.copy == 0:\n",
        "            self.copy_model()\n",
        "\n",
        "        if self.memory_sample_size > self.num_in_queue:\n",
        "            return\n",
        "\n",
        "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
        "        STATE = STATE.to(self.device)\n",
        "        ACTION = ACTION.to(self.device)\n",
        "        REWARD = REWARD.to(self.device)\n",
        "        STATE2 = STATE2.to(self.device)\n",
        "        DONE = DONE.to(self.device)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Double Q-Learning target is Q*(S, A) <- r + Î³ max_a Q_target(S', a)\n",
        "        target = REWARD + torch.mul((self.gamma * \n",
        "                                    self.target_net(STATE2).max(1).values.unsqueeze(1)), \n",
        "                                    1 - DONE)\n",
        "\n",
        "        current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
        "        loss = self.l1(current, target)\n",
        "        loss.backward() # Compute gradients\n",
        "        self.optimizer.step() # Backpropagate error\n",
        "\n",
        "        self.exploration_rate *= self.exploration_decay\n",
        "        \n",
        "        # Makes sure that exploration rate is always at least 'exploration min'\n",
        "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P2Y_1Boxns-f"
      },
      "outputs": [],
      "source": [
        "def show_state(env, ep=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_n_TF3dJXdG"
      },
      "source": [
        "Super mario gym offers us mario in different variations. Here we will test the learning of our AI for v3.\n",
        "---\n",
        "v0 is the standard Mario Versio whereas v3 is simplified mario reduced to pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Rg5bSYHZLhuD"
      },
      "outputs": [],
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
        "env = make_env(env)\n",
        "state=env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QsSZtMTHHHB9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_state(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qEYmdx0tnvCT",
        "outputId": "074f1bdd-3cf6-4a3d-b0bb-904aac81173b"
      },
      "outputs": [],
      "source": [
        "def run(training_mode, pretrained): \n",
        "    observation_space = env.observation_space.shape\n",
        "    action_space = env.action_space.n\n",
        "    agent = DDQNAgent(state_space=observation_space,\n",
        "                     action_space=action_space,\n",
        "                     max_memory_size=30000,\n",
        "                     batch_size=32,\n",
        "                     gamma=0.90,\n",
        "                     lr=0.00025,\n",
        "                     dropout=0.,\n",
        "                     exploration_max=1.0,\n",
        "                     exploration_min=0.02,\n",
        "                     exploration_decay=0.99,\n",
        "                     pretrained=pretrained)\n",
        "    \n",
        "    num_episodes = 1000\n",
        "    env.reset()\n",
        "    total_rewards = []\n",
        "    \n",
        "    for ep_num in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        state = torch.Tensor([state])\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            if not training_mode:\n",
        "                show_state(env, ep_num)\n",
        "            action = agent.act(state)\n",
        "            steps += 1\n",
        "            \n",
        "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
        "            total_reward += reward\n",
        "            state_next = torch.Tensor([state_next])\n",
        "            reward = torch.tensor([reward]).unsqueeze(0)\n",
        "            \n",
        "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
        "            \n",
        "            if training_mode:\n",
        "                agent.remember(state, action, reward, state_next, terminal)\n",
        "                agent.experience_replay()\n",
        "            \n",
        "            state = state_next\n",
        "            if terminal:\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
        "        num_episodes += 1      \n",
        "    \n",
        "    if training_mode:\n",
        "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.ending_position, f)\n",
        "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.num_in_queue, f)\n",
        "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
        "            pickle.dump(total_rewards, f)\n",
        "    \n",
        "        torch.save(agent.local_net.state_dict(), \"dq1.pt\")\n",
        "        torch.save(agent.target_net.state_dict(), \"dq2.pt\")\n",
        "          \n",
        "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
        "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
        "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
        "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
        "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    if num_episodes > 50:\n",
        "        plt.title(\"Episodes trained vs. Average Rewards (per 50 eps)\")\n",
        "        plt.plot([0 for _ in range(50)] + \n",
        "                 np.convolve(total_rewards, np.ones((50,))/50, mode=\"valid\").tolist())\n",
        "        plt.show()\n",
        "\n",
        "run(training_mode=True, pretrained=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mario2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
